import asyncio
import concurrent
import gc
import hashlib
import json
import logging
import os
import uuid
import warnings
from copy import deepcopy
from datetime import datetime
import sys 
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
import pytz
from typing import Any, Dict, Optional , List, Tuple
import logging
from pathlib import Path
from AgentMem.logger import get_logger
logger = get_logger(__name__, filename="AMem.log")

from pydantic import ValidationError

from AgentMem.configs.base import MemoryConfig
from AgentMem.memory.base import MemoryBase
from AgentMem.memory.setup import AgentMem_dir, setup_config
from AgentMem.memory.storage import SQLiteManager
from AgentMem.memory.telemetry import capture_event
from AgentMem.utils.factory import EmbedderFactory, LlmFactory, VectorStoreFactory
from AgentMem.memory.utils import (
    get_fact_retrieval_messages,
    parse_messages,
    parse_vision_messages,
    process_telemetry_filters,
    remove_code_blocks,
)
# å»ºè®®åœ¨ configs/prompts.py ä¸­å®šä¹‰
SYMBOL_EXTRACTION_PROMPT = """
You are an intelligent knowledge extractor. Your task is to analyze the following memory chunk (a piece of user-agent conversation) and extract crucial structured information in JSON format.

Constraints:
1. Identify all main [Entities] (e.g., people, projects, places, dates).
2. Identify the [Core Relationship] or [Action] that links the entities (e.g., 'discusses', 'scheduled for', 'completed').
3. Extract the [Time Context] (exact date, day of the week, or relative term like 'next week'). If none, use 'N/A'.
4. Do not include any explanation or extra text. Output ONLY the JSON object.

Example Input: "User: Hey, did we finalize the Q3 marketing plan review? Agent: Yes, that was completed last Tuesday, November 15th, by Sarah and David."
Example Output: 
{{
  "Entities": ["Q3 marketing plan review", "Sarah", "David"],
  "Core Relationship": "completed",
  "Time Context": "November 15th"
}}

---
Memory Chunk: 
{memory_chunk}
"""

RE_RANKING_VALIDATION_PROMPT = """
You are a highly logical Re-ranker and Validator. A user asked the question: '{query}'.
The retrieval system provided the following candidate memory chunks (with their respective semantic relevance scores).

Candidate Memories:
{candidate_memories}

The initial vector search found these memories to be semantically relevant. However, you must now apply logical and symbolic constraints (based on entities, time, and relationships) to filter and re-rank them.

Instructions:
1. Filter out any memories that are factually contradicted by a high-ranking memory, or that are clearly irrelevant to the specific entities/time mentioned in the query.
2. Rank the remaining memories from 1 (Most Relevant) to N.

Output ONLY the final filtered and re-ranked memories in a JSON object with key "filtered_memories.If a memory must be discarded, exclude it.":
{{
  "filtered_memories": [
    {{ "rank": 1, "memory_id": "id-xyz", "reasoning": "Directly mentions the project status." }},
    {{ "rank": 2, "memory_id": "id-abc", "reasoning": "Provides background context." }}
  ]
}}
"""

class Memory(MemoryBase):
    def __init__(self, config: MemoryConfig = MemoryConfig()):
        setup_config()
        self.config = config

        # embedding / vector store / llm
        self.embedding_model = EmbedderFactory.create(
            self.config.embedder.provider,
            self.config.embedder.config,
            self.config.vector_store.config,
        )
        self.vector_store = VectorStoreFactory.create(
            self.config.vector_store.provider, self.config.vector_store.config
        )
        self.llm = LlmFactory.create(self.config.llm.provider, self.config.llm.config)

        # sqlite å†å²æ•°æ®åº“
        self.db = SQLiteManager(self.config.history_db_path)

        # collection_name & path
        self.collection_name = self.config.vector_store.config.collection_name or "mem0migrations"
        if self.config.vector_store.provider in ["faiss", "qdrant"]:
            provider_path = f"migrations_{self.config.vector_store.provider}"
            self.config.vector_store.config.path = os.path.join(AgentMem_dir, provider_path)
            os.makedirs(self.config.vector_store.config.path, exist_ok=True)

        # å›¾å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
        self.enable_graph = False
        self.graph = None
        if self.config.graph_store.config:
            if self.config.graph_store.provider == "memgraph":
                from AgentMem.memory.memgraph_memory import MemoryGraph
            elif self.config.graph_store.provider == "neptune":
                from AgentMem.graphs.neptune.main import MemoryGraph
            else:
                from AgentMem.memory.graph_memory import MemoryGraph

            self.graph = MemoryGraph(self.config)
            self.enable_graph = True

        # telemetry
        capture_event("AgentMem.init", self, {"sync_type": "sync"})

    @classmethod
    def from_config(cls, config_dict: Dict[str, Any]):
        try:
            # å…¼å®¹æ—§é…ç½®
            if "graph_store" in config_dict and "vector_store" not in config_dict and "embedder" in config_dict:
                config_dict["vector_store"] = {
                    "config": {
                        "embedding_model_dims": config_dict["embedder"]["config"]["embedding_dims"]
                    }
                }
            config = MemoryConfig(**config_dict)
        except ValidationError as e:
            raise ValueError(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
        return cls(config)
    def _create_memory(self, data, existing_embeddings, metadata=None):
        logger.debug(f"Creating memory with data={data[:50] if isinstance(data, str) else data}")
        if data in existing_embeddings:
            embeddings = existing_embeddings[data]
        else:
            embeddings = self.embedding_model.embed(data, memory_action="add")
        memory_id = str(uuid.uuid4())
        payload = metadata or {}
        payload["data"] = data  # ç”¨äºæ£€ç´¢æ—¶è·å–æ–‡æœ¬
        payload["text"] = data   # å…¼å®¹æ€§å­—æ®µ
        payload["hash"] = hashlib.md5(data.encode()).hexdigest()
        payload["created_at"] = datetime.now(pytz.timezone("US/Pacific")).isoformat()
        
        logger.debug(f"Inserting memory {memory_id} with user_id={payload.get('user_id')}")

        self.vector_store.insert(
            vectors=[embeddings],
            ids=[memory_id],
            payloads=[payload],
        )
        self.db.add_history(
            memory_id,
            None,
            data,
            "ADD",
            created_at=metadata.get("created_at"),
            actor_id=metadata.get("actor_id"),
            role=metadata.get("role"),
        )
        capture_event("mem0._create_memory", self, {"memory_id": memory_id, "sync_type": "sync"})
        return memory_id
    def add(self, messages: str, user_id: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        """
        [åˆ›æ–°ç‚¹ 1: å†™å…¥ - åŒé‡ç¼–ç ]ï¼šå­˜å‚¨æ–‡æœ¬å‘é‡ï¼Œå¹¶åŒæ—¶è¿›è¡Œç¬¦å·æå–å’Œå›¾å­˜å‚¨ã€‚
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯æ˜¯å­—å…¸æ ¼å¼ {"role": "user", "content": "...", ...}
            user_id: ç”¨æˆ·ID
            metadata: é¢å¤–çš„å…ƒæ•°æ®
        """
        
        # # 1. å‘é‡å­˜å‚¨ (Vector Storage)
        # logger.info(f"è¾“å…¥æ–‡æœ¬:{text}")
        # for message in text:
        #     embedding = self.embedding_model.embed(message)
        # embedding = self.embedding_model.embed(text)
        # # logger.info(f"åµŒå…¥æ–‡æœ¬:{embedding}")
        # self.vector_store.insert(
        #     vectors=[embedding],
        #     ids=[memory_id],
        #     payloads=[{"id": memory_id, "user_id": user_id, "text": text, **(metadata or {})}],
        # )
        for message_dict in messages:
                if (
                    not isinstance(message_dict, dict)
                    or message_dict.get("role") is None
                    or message_dict.get("content") is None
                ):
                    logger.warning(f"Skipping invalid message format: {message_dict}")
                    continue

                if message_dict["role"] == "system":
                    continue

                per_msg_meta = deepcopy(metadata) or {}
                per_msg_meta["user_id"] = user_id  # ç¡®ä¿ user_id è¢«åŒ…å«
                per_msg_meta["role"] = message_dict["role"]

                actor_name = message_dict.get("name")
                if actor_name:
                    per_msg_meta["actor_id"] = actor_name

                msg_content = message_dict["content"]
                msg_embeddings = self.embedding_model.embed(msg_content, "add")
                mem_id = self._create_memory(msg_content, msg_embeddings, per_msg_meta)

        # 2. ç¬¦å·æå–ä¸å›¾å­˜å‚¨ (Symbol Extraction and Graph Storage)
        if self.enable_graph and self.graph:
            try:
                # ä½¿ç”¨ LLM æå–ç¬¦å·ä¿¡æ¯
                prompt = SYMBOL_EXTRACTION_PROMPT.format(memory_chunk=messages)
                
                # å‡è®¾ self.llm æœ‰ä¸€ä¸ª generate_text æ–¹æ³•
                response = self.llm.generate_response(prompt)
                
                # è§£æ LLM çš„ JSON è¾“å‡º
                symbolic_data = json.loads(response.strip())
                
                entities = symbolic_data.get("Entities", [])
                relationship = symbolic_data.get("Core Relationship", "mentions")
                time_context = symbolic_data.get("Time Context", None)

                # å°†ä¿¡æ¯å†™å…¥å›¾æ•°æ®åº“ (Graph Store)
                # å»ºç«‹å›¾èŠ‚ç‚¹å’Œå…³ç³»ï¼š(Entity A) -[RELATIONSHIP]-> (Entity B)
                if entities:
                    # åˆ›å»ºä¸€ä¸ªä»£è¡¨æ­¤è®°å¿†ç‰‡æ®µçš„ä¸­å¿ƒèŠ‚ç‚¹
                    # self.graph.add_memory_node(memory_id, text, user_id, time_context) 
                    
                    # è¿æ¥å®ä½“åˆ°è®°å¿†èŠ‚ç‚¹
                    for entity in entities:
                        # å‡è®¾ graph.add_entity_link æ–¹æ³•å¯ä»¥åˆ›å»ºå®ä½“èŠ‚ç‚¹å’Œå…³ç³»
                        self.graph._add_entities(entity, {"user_id":user_id,"agent_id":mem_id},relationship) 

                print(f"Added memory {mem_id} to vector store and extracted symbols.")
            except Exception as e:
                print(f"Warning: Failed to extract or store symbolic data for memory {mem_id}. Error: {e}")

        # 3. å†å²æ•°æ®åº“å­˜å‚¨ (History DB Storage)
       
        return mem_id


    def search(self, user_id: str, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        [åˆ›æ–°ç‚¹ 2: æ£€ç´¢ - åŒé€šé“æ··åˆæ£€ç´¢ä¸é‡æ’åº]ï¼šå‘é‡æ£€ç´¢ -> ç¬¦å·è¿‡æ»¤/é‡æ’åº -> ç­”æ¡ˆç”Ÿæˆã€‚
        """
        # 1. è¯­ä¹‰é€šé“ï¼šåˆæ¬¡å‘é‡æ£€ç´¢ (Vector Search)
        query_embedding = self.embedding_model.embed(query)
        
        candidate_results = self.vector_store.search(
            query = query, 
            vectors = query_embedding,
            limit = limit * 3,  # æé«˜å¬å›é™åˆ¶ï¼Œä»¥ä¾¿åç»­è¿‡æ»¤
            filters={"user_id": user_id}
        )
        
        if not candidate_results:
            logger.warning(f"No search results found for query: {query}, user_id: {user_id}")
            return []
        
        # æ ¼å¼åŒ–å€™é€‰è®°å¿†ï¼Œç”¨äº LLM é‡æ’åº
        candidate_memories = []
        for result in candidate_results:
            # å¤„ç†ä¸åŒç±»å‹çš„ç»“æœæ ¼å¼
            # 1. å¦‚æœæ˜¯å¯¹è±¡å±æ€§ (result.id, result.payload, result.score)
            # 2. æˆ–è€…æ˜¯å­—å…¸æ ¼å¼çš„ payload
            # 3. payload ä¸­å¯èƒ½ç”¨ 'data' é”®å­˜å‚¨æ–‡æœ¬
            
            try:
                # è·å– payloadï¼ˆåŒ…å«æ‰€æœ‰å…ƒæ•°æ®ï¼‰
                if hasattr(result, 'payload') and isinstance(result.payload, dict):
                    payload = result.payload.copy()  # å¤åˆ¶ payload ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
                elif isinstance(result, dict):
                    payload = result.copy()
                else:
                    logger.debug(f"Skipping result: invalid type {type(result)}")
                    continue
                
                # è·å– id
                mem_id = payload.pop('id', None)
                if mem_id is None:
                    mem_id = getattr(result, 'id', None)
                    if mem_id is None:
                        logger.debug(f"Skipping result: no id found")
                        continue
                
                # è·å– score
                score = payload.pop('score', None)
                if score is None:
                    score = getattr(result, 'score', 0)
                
                # è·å– text (å°è¯•å¤šä¸ªå¯èƒ½çš„é”®)
                text = payload.pop('data', None) or payload.pop('text', None) or payload.pop('memory', '')
                
                # è·³è¿‡ç©ºçš„æˆ–æ— æ•ˆçš„è®°å¿†
                if not text or not mem_id:
                    logger.debug(f"Skipping invalid result: id={mem_id}, text={text[:50] if text else 'empty'}")
                    continue
                
                candidate_memories.append({
                    "memory_id": mem_id,  # ä½¿ç”¨ memory_id é”®åä»¥å…¼å®¹ LLM é‡æ’åº
                    "id": mem_id,  # åŒæ—¶ä¿ç•™ id é”®
                    "text": text,
                    "score": round(float(score), 4) if score is not None else 0.0,
                    # ä¿ç•™æ‰€æœ‰å…¶ä»–å…ƒæ•°æ®
                    **payload
                })
            except Exception as e:
                logger.error(f"Error processing result: {e}, result type: {type(result)}")
                continue
        
        # 2. ç¬¦å·é€šé“ä¸æ¨ç†ï¼šé‡æ’åºä¸éªŒè¯ (Re-ranking and Validation)
        
        # 2a. æç¤ºè¯æ³¨å…¥
        prompt = RE_RANKING_VALIDATION_PROMPT.format(
            query=query,
            candidate_memories=json.dumps(candidate_memories, indent=2)
        )
        
        # 2b. LLM æ‰§è¡Œé€»è¾‘æ¨ç†å’Œé‡æ’åº
        try:
            # response = self.llm.generate_response(prompt)
            response = self.llm.generate_response(
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": query},
            ],
            response_format={"type": "json_object"},
        )
            
            response = eval(response) if isinstance(response, str) else response
            logger.info(
    f"LLM Re-ranking response: {response}, "
    f"type(response): {type(response)}, "
    f"filtered_memories: {response.get('filtered_memories', []) if isinstance(response, dict) else 'N/A'}"
)
            # å‡è®¾ LLM è¿”å› JSON æ ¼å¼çš„é‡æ’åºç»“æœ
            # re_ranked_list: List[Dict[str, Any]] = json.loads(response["filtered_memories"].strip())
            re_ranked_list: List[Dict[str, Any]] = response.get("filtered_memories", [])
            logger.info(f"Parsed re-ranked list: {re_ranked_list}")
            # 3. ç»“æœæ•´åˆä¸æœ€ç»ˆè¾“å‡º (Final Integration)
            final_memories = []
            logger.info(f"candidate_memories: {candidate_memories}")
            # åˆ›å»ºä¸€ä¸ª ID åˆ°åŸå§‹è®°å¿†çš„æ˜ å°„
            id_to_memory = {mem['id']: mem for mem in candidate_memories}
            logger.info(f"id_to_memory: {id_to_memory}")
            for item in re_ranked_list[:limit]: # é™åˆ¶æœ€ç»ˆè¾“å‡ºæ•°é‡
                mem_id = item.get('memory_id')
                logger.info(f"Processing re-ranked memory ID: {mem_id}")
                if mem_id and mem_id in id_to_memory:
                    # æŸ¥æ‰¾åŸå§‹è®°å¿†ï¼ŒåŒ…å«æ‰€æœ‰å…ƒæ•°æ®
                    original_memory = id_to_memory[mem_id]
                    # ä¿ç•™æ‰€æœ‰åŸæœ‰å­—æ®µï¼Œåªæ·»åŠ  rank_reasoning
                    final_memory = original_memory.copy()
                    final_memory["rank_reasoning"] = item.get('reasoning')
                    final_memories.append(final_memory)
            logger.info(f"Final re-ranked memories: {final_memories}")
            # å¦‚æœ LLM é‡æ’åºå¤±è´¥æˆ–è¿”å›ç©ºï¼Œåˆ™å›é€€åˆ°åŸå§‹å‘é‡æ£€ç´¢ç»“æœ
            if not final_memories and candidate_memories:
                logger.warning("LLM re-ranking failed or returned empty, falling back to top vector results")
                return [mem.copy() for mem in candidate_memories[:limit]]
                
            return final_memories
            
        except Exception as e:
            logger.error(f"LLM Re-ranking failed with error: {e}. Falling back to top vector results.")
            # å¤±è´¥å›é€€æœºåˆ¶ - ä¸º fallback ç»“æœæ·»åŠ  rank_reasoning
            return [
                {**mem.copy(), "rank_reasoning": "Fallback (System Error)"}
                for mem in candidate_memories[:limit]
            ]
    # delete, get, get_all, history, update
    def delete(self, user_id: str, memory_id: str) -> bool:
        # åˆ é™¤å‘é‡å­˜å‚¨ä¸­çš„è®°å¿†
        self.vector_store.delete(ids=[memory_id])
        
        # åˆ é™¤å†å²æ•°æ®åº“ä¸­çš„è®°å¿†
        self.db.delete_memory(user_id, memory_id)
        
        # åˆ é™¤å›¾å­˜å‚¨ä¸­çš„è®°å¿†èŠ‚ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_graph and self.graph:
            self.graph.delete_memory_node(memory_id)
    def delete_all(self, user_id: Optional[str] = None, agent_id: Optional[str] = None, run_id: Optional[str] = None):
        """
        Delete all memories.

        Args:
            user_id (str, optional): ID of the user to delete memories for. Defaults to None.
            agent_id (str, optional): ID of the agent to delete memories for. Defaults to None.
            run_id (str, optional): ID of the run to delete memories for. Defaults to None.
        """
        filters: Dict[str, Any] = {}
        if user_id:
            filters["user_id"] = user_id
        if agent_id:
            filters["agent_id"] = agent_id
        if run_id:
            filters["run_id"] = run_id

        if not filters:
            raise ValueError(
                "At least one filter is required to delete all memories. If you want to delete all memories, use the `reset()` method."
            )

        keys, encoded_ids = process_telemetry_filters(filters)
        capture_event("mem0.delete_all", self, {"keys": keys, "encoded_ids": encoded_ids, "sync_type": "sync"})
        memories = self.vector_store.list(filters=filters)[0]
        # for memory in memories:
        #     self._delete_memory(memory.id)
        for memory in memories:
            try:
                self._delete_memory(memory.id)
            except IndexError as e:
                logger.warning(f"Failed to delete memory {memory.id}: {e}")
        logger.info(f"Deleted {len(memories)} memories")

        return {"message": "Memories deleted successfully!"}
    def _delete_memory(self, memory_id: str):
        existing_memory = self.vector_store.get(vector_id=memory_id)
        prev_value = existing_memory.payload["data"]
        self.vector_store.delete(vector_id=memory_id)
        self.db.add_history(
            memory_id,
            prev_value,
            None,
            "DELETE",
            actor_id=existing_memory.payload.get("actor_id"),
            role=existing_memory.payload.get("role"),
            is_deleted=1,
        )
        # capture_event("mem0._delete_memory", self, {"memory_id": memory_id, "sync_type": "sync"})
        return memory_id
    def get(self, user_id: str, memory_id: str) -> Optional[Dict[str, Any]]:
        return self.db.get_memory(user_id, memory_id)
    def get_all(self, user_id: str) -> List[Dict[str, Any]]:
        return self.db.get_all_memories(user_id)
    def history(self, user_id: str, limit: int = 10) -> List[Dict[str, Any]]:
        return self.db.get_recent_memories(user_id, limit)
    def update(self, user_id: str, memory_id: str, new_text: str, new_metadata: Optional[Dict[str, Any]] = None) -> bool:
        # æ›´æ–°å‘é‡å­˜å‚¨ä¸­çš„è®°å¿†
        new_embedding = self.embedding_model.embed(new_text)
        self.vector_store.update(
            ids=[memory_id],
            vectors=[new_embedding],
            payloads=[{"user_id": user_id, "text": new_text, **(new_metadata or {})}]
        ) 
# --- 4. Main æµ‹è¯•é€»è¾‘ ---
def main():
    config = {
    "llm": {
         "provider": "vllm",
            "config": {
            "model": "Qwen/Qwen2.5-7B-Instruct",
            "vllm_base_url": "http://localhost:8000/v1",
            "api_key": "vllm-api-key",
            "temperature": 0,
            "max_tokens": 2000,
            },
    },
        "embedder": {"provider": "huggingface", "config": {"model": "all-MiniLM-L6-v2"}},
    # "vector_store": {
    #     "provider": "qdrant",
    #     "config": {"collection_name": "vllm_memories", "host": "localhost", "port": 6333},
    # },
    #  "graph_store": {
    #         "provider": "neo4j",# or neo4j-community
    #         "config": {
    #             "url": "bolt://localhost:7687",
    #             "username": "neo4j", # or neo4j
    #             "password": "Neo4j2025",
    #             "database": "neo4j",
    #         }
    #     }
}
    print("--- ğŸ§  HRNSM è®°å¿†ç³»ç»Ÿå¯¹è¯æ–‡æœ¬æµ‹è¯• ---")
    memory = Memory.from_config(config)
    user_id = "test_user"

    # --- æµ‹è¯• ADD (å†™å…¥å¯¹è¯æ–‡æœ¬) ---
    print("\n## ğŸ“ 1. æµ‹è¯• ADD (å†™å…¥å¯¹è¯æ–‡æœ¬)")
    
    # ç¤ºä¾‹å¯¹è¯æ–‡æœ¬ 1: ä¸ªäººäº‹ä»¶
    dialogue_1 = {"role":"user","content":"Melanie: Hey Caroline, since we last chatted, I've had a lot of things happening to me. I ran a charity race for mental health last Saturday â€“ it was really rewarding. Really made me think about taking care of our minds."}
    # memory.add(user_id, dialogue_1)
    
    # ç¤ºä¾‹å¯¹è¯æ–‡æœ¬ 2: å…³é”®äº‹å®
    dialogue_2 = {"role":"user","content": "Melanie: The Q4 Report review is scheduled for next Monday. Caroline: Perfect, I'll block out time for that."}
    
     # new_id = memory.add(user_id, dialogue_2)
    
    print(f"-> å·²å­˜å‚¨ä¸¤ä¸ªå¯¹è¯ç‰‡æ®µ (ä¸€ä¸ªå…³äºæ…ˆå–„è·‘ï¼Œä¸€ä¸ªå…³äº Q4 æŠ¥å‘Š)")
    messages = [dialogue_1, dialogue_2]
    memory.add(messages, user_id=user_id, metadata={"timestamp":"Jan28 2026"})
    # éªŒè¯ç¬¦å·æå–æ˜¯å¦è¢«è°ƒç”¨
    # print(f"-> éªŒè¯å›¾å­˜å‚¨è°ƒç”¨: {memory.graph.add_memory_node.called}")

    print("\n---")
    
    # --- æµ‹è¯• SEARCH (æ£€ç´¢ï¼šé€»è¾‘è¿‡æ»¤å™ªå£°) ---
    print("## ğŸ” 2. æµ‹è¯• SEARCH (åŒé€šé“æ··åˆæ£€ç´¢ä¸é€»è¾‘è¿‡æ»¤)")
    # æŸ¥è¯¢ï¼šå¯»æ‰¾ä¸€ä¸ªæ˜ç¡®çš„æ—¥ç¨‹/äº‹å®
    query = "When is the Q4 Report review scheduled?"
    
    results = memory.search(user_id, query, limit=2)
    
    print(f"-> æœç´¢æŸ¥è¯¢: '{query}'")
    print(f"-> **æœ€ç»ˆç»“æœ (Top 2):**")
    
    if not results:
        print("æœç´¢å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœã€‚")
        return

    for i, result in enumerate(results):
        print(f"\n- Rank {i+1}: (Score: {result['score']})")
        print(f"  Text: {result['text']}")
        print(f"  Logic: {result['rank_reasoning']}")
        print(f"  result:  {result}")
        
    # é¢„æœŸåˆ†æï¼š
    # å‘é‡æ£€ç´¢ä¼šè¿”å› M2 (Q4) å’Œ M1 (æ…ˆå–„è·‘)
    # LLM é‡æ’åºä¼šåˆ¤æ–­ M2 ç›´æ¥å›ç­”äº†é—®é¢˜ï¼Œå°†å…¶æ’åœ¨ç¬¬ä¸€ï¼›M1 è™½è¯­ä¹‰ç›¸å…³ä½†ä¸æ˜¯äº‹å®ç­”æ¡ˆï¼Œæ’åœ¨ç¬¬äºŒã€‚
    # å™ªéŸ³ M3 (Q1 æŠ¥å‘Š) è¢« LLM é€»è¾‘è¿‡æ»¤ã€‚

if __name__ == "__main__":
    main()