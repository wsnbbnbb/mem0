# mem0/memory/main.py
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from copy import deepcopy
from typing import Any, Dict, Optional , List, Tuple
import uuid
import json
import logging
from pathlib import Path
# from ..logger import get_logger
# # ---------- æ—¥å¿—é…ç½® ----------
LOG_DIR = Path(__file__).parent / "logs"
LOG_DIR.mkdir(exist_ok=True)  # å¦‚æœ logs æ–‡ä»¶å¤¹ä¸å­˜åœ¨å°±åˆ›å»º

LOG_FILE = LOG_DIR / "AMemo.log"

# # åˆ›å»º logger
log = logging.getLogger("AMemoLogger")
log.setLevel(logging.INFO)  # è¾“å‡ºçº§åˆ«

# æ§åˆ¶å° handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter("[%(asctime)s] [%(levelname)s] %(message)s")
console_handler.setFormatter(console_formatter)

# æ–‡ä»¶ handler
file_handler = logging.FileHandler(LOG_FILE, encoding="utf-8")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(console_formatter)

# æ·»åŠ  handler
# log.addHandler(console_handler)
log.addHandler(file_handler)

from pydantic import ValidationError

from AgentMem.configs.base import MemoryConfig
from AgentMem.memory.base import MemoryBase
from AgentMem.memory.setup import mem0_dir, setup_config
from AgentMem.memory.storage import SQLiteManager
from AgentMem.memory.telemetry import capture_event
from AgentMem.utils.factory import EmbedderFactory, LlmFactory, VectorStoreFactory

# å»ºè®®åœ¨ configs/prompts.py ä¸­å®šä¹‰
SYMBOL_EXTRACTION_PROMPT = """
You are an intelligent knowledge extractor. Your task is to analyze the following memory chunk (a piece of user-agent conversation) and extract crucial structured information in JSON format.

Constraints:
1. Identify all main [Entities] (e.g., people, projects, places, dates).
2. Identify the [Core Relationship] or [Action] that links the entities (e.g., 'discusses', 'scheduled for', 'completed').
3. Extract the [Time Context] (exact date, day of the week, or relative term like 'next week'). If none, use 'N/A'.
4. Do not include any explanation or extra text. Output ONLY the JSON object.

Example Input: "User: Hey, did we finalize the Q3 marketing plan review? Agent: Yes, that was completed last Tuesday, November 15th, by Sarah and David."
Example Output: 
{{
  "Entities": ["Q3 marketing plan review", "Sarah", "David"],
  "Core Relationship": "completed",
  "Time Context": "November 15th"
}}

---
Memory Chunk: 
{memory_chunk}
"""

RE_RANKING_VALIDATION_PROMPT = """
You are a highly logical Re-ranker and Validator. A user asked the question: '{query}'.
The retrieval system provided the following candidate memory chunks (with their respective semantic relevance scores).

Candidate Memories:
{candidate_memories}

The initial vector search found these memories to be semantically relevant. However, you must now apply logical and symbolic constraints (based on entities, time, and relationships) to filter and re-rank them.

Instructions:
1. Filter out any memories that are factually contradicted by a high-ranking memory, or that are clearly irrelevant to the specific entities/time mentioned in the query.
2. Rank the remaining memories from 1 (Most Relevant) to N.

Output ONLY the final, filtered, and re-ranked list of memories in the following JSON format. If a memory must be discarded, exclude it.

Example Output:
[
  {{ "rank": 1, "memory_id": "id-xyz", "reasoning": "Directly mentions the project status and time requested." }},
  {{ "rank": 2, "memory_id": "id-abc", "reasoning": "Provides background context about the project's inception." }}
]
"""

class Memory(MemoryBase):
    def __init__(self, config: MemoryConfig = MemoryConfig()):
        setup_config()
        self.config = config

        # embedding / vector store / llm
        self.embedding_model = EmbedderFactory.create(
            self.config.embedder.provider,
            self.config.embedder.config,
            self.config.vector_store.config,
        )
        self.vector_store = VectorStoreFactory.create(
            self.config.vector_store.provider, self.config.vector_store.config
        )
        self.llm = LlmFactory.create(self.config.llm.provider, self.config.llm.config)

        # sqlite å†å²æ•°æ®åº“
        self.db = SQLiteManager(self.config.history_db_path)

        # collection_name & path
        self.collection_name = self.config.vector_store.config.collection_name or "mem0migrations"
        if self.config.vector_store.provider in ["faiss", "qdrant"]:
            provider_path = f"migrations_{self.config.vector_store.provider}"
            self.config.vector_store.config.path = os.path.join(mem0_dir, provider_path)
            os.makedirs(self.config.vector_store.config.path, exist_ok=True)

        # å›¾å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
        self.enable_graph = False
        self.graph = None
        if self.config.graph_store.config:
            if self.config.graph_store.provider == "memgraph":
                from AgentMem.memory.memgraph_memory import MemoryGraph
            elif self.config.graph_store.provider == "neptune":
                from AgentMem.graphs.neptune.main import MemoryGraph
            else:
                from AgentMem.memory.graph_memory import MemoryGraph

            self.graph = MemoryGraph(self.config)
            self.enable_graph = True

        # telemetry
        capture_event("AgentMem.init", self, {"sync_type": "sync"})

    @classmethod
    def from_config(cls, config_dict: Dict[str, Any]):
        try:
            # å…¼å®¹æ—§é…ç½®
            if "graph_store" in config_dict and "vector_store" not in config_dict and "embedder" in config_dict:
                config_dict["vector_store"] = {
                    "config": {
                        "embedding_model_dims": config_dict["embedder"]["config"]["embedding_dims"]
                    }
                }
            config = MemoryConfig(**config_dict)
        except ValidationError as e:
            raise ValueError(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
        return cls(config)
    
    def add(self, user_id: str, text: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        """
        [åˆ›æ–°ç‚¹ 1: å†™å…¥ - åŒé‡ç¼–ç ]ï¼šå­˜å‚¨æ–‡æœ¬å‘é‡ï¼Œå¹¶åŒæ—¶è¿›è¡Œç¬¦å·æå–å’Œå›¾å­˜å‚¨ã€‚
        """
        memory_id = str(uuid.uuid4())
        
        # 1. å‘é‡å­˜å‚¨ (Vector Storage)
        embedding = self.embedding_model.embed(text)
        self.vector_store.insert(
            vectors=[embedding],
            payloads=[{"id": memory_id, "user_id": user_id, "text": text, **(metadata or {})}],
            ids=[memory_id]
        )

        # 2. ç¬¦å·æå–ä¸å›¾å­˜å‚¨ (Symbol Extraction and Graph Storage)
        if self.enable_graph and self.graph:
            try:
                # ä½¿ç”¨ LLM æå–ç¬¦å·ä¿¡æ¯
                prompt = SYMBOL_EXTRACTION_PROMPT.format(memory_chunk=text)
                
                # å‡è®¾ self.llm æœ‰ä¸€ä¸ª generate_text æ–¹æ³•
                response = self.llm.generate_response(prompt)
                
                # è§£æ LLM çš„ JSON è¾“å‡º
                symbolic_data = json.loads(response.strip())
                
                entities = symbolic_data.get("Entities", [])
                relationship = symbolic_data.get("Core Relationship", "mentions")
                time_context = symbolic_data.get("Time Context", None)

                # å°†ä¿¡æ¯å†™å…¥å›¾æ•°æ®åº“ (Graph Store)
                # å»ºç«‹å›¾èŠ‚ç‚¹å’Œå…³ç³»ï¼š(Entity A) -[RELATIONSHIP]-> (Entity B)
                if entities:
                    # åˆ›å»ºä¸€ä¸ªä»£è¡¨æ­¤è®°å¿†ç‰‡æ®µçš„ä¸­å¿ƒèŠ‚ç‚¹
                    # self.graph.add_memory_node(memory_id, text, user_id, time_context) 
                    
                    # è¿æ¥å®ä½“åˆ°è®°å¿†èŠ‚ç‚¹
                    for entity in entities:
                        # å‡è®¾ graph.add_entity_link æ–¹æ³•å¯ä»¥åˆ›å»ºå®ä½“èŠ‚ç‚¹å’Œå…³ç³»
                        self.graph._add_entities(entity, {"user_id":user_id,"agent_id":memory_id},relationship) 

                print(f"Added memory {memory_id} to vector store and extracted symbols.")
            except Exception as e:
                print(f"Warning: Failed to extract or store symbolic data for memory {memory_id}. Error: {e}")

        # 3. å†å²æ•°æ®åº“å­˜å‚¨ (History DB Storage)
        self.db.add_history(user_id, memory_id, text, metadata)

        capture_event("AgentMem.add", self, {"user_id": user_id, "text_len": len(text)})
        return memory_id


    def search(self, user_id: str, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        [åˆ›æ–°ç‚¹ 2: æ£€ç´¢ - åŒé€šé“æ··åˆæ£€ç´¢ä¸é‡æ’åº]ï¼šå‘é‡æ£€ç´¢ -> ç¬¦å·è¿‡æ»¤/é‡æ’åº -> ç­”æ¡ˆç”Ÿæˆã€‚
        """
        # 1. è¯­ä¹‰é€šé“ï¼šåˆæ¬¡å‘é‡æ£€ç´¢ (Vector Search)
        query_embedding = self.embedding_model.embed(query)
        
        # å‡è®¾ vector_store.search è¿”å› (text, metadata, score)
        candidate_results: List[Tuple[str, Dict[str, Any], float]] = self.vector_store.search(
            query = query, 
            vectors = query_embedding,
            limit = limit * 3,  # æé«˜å¬å›é™åˆ¶ï¼Œä»¥ä¾¿åç»­è¿‡æ»¤
            filters={"user_id": user_id}
        )
        if not candidate_results:
            return []
        # print(f"{candidate_results[0]}\n--------------------")
        # æ ¼å¼åŒ–å€™é€‰è®°å¿†ï¼Œç”¨äº LLM é‡æ’åº
        candidate_memories = []
        for result in candidate_results:
            id = result.id
            text = result.payload.get("text", "")
            score = result.score 
            candidate_memories.append({
                "memory_id": id,
                "text": text,
                "score": round(score, 4) 
            })
        
        # 2. ç¬¦å·é€šé“ä¸æ¨ç†ï¼šé‡æ’åºä¸éªŒè¯ (Re-ranking and Validation)
        
        # 2a. æç¤ºè¯æ³¨å…¥
        prompt = RE_RANKING_VALIDATION_PROMPT.format(
            query=query,
            candidate_memories=json.dumps(candidate_memories, indent=2)
        )
        
        # 2b. LLM æ‰§è¡Œé€»è¾‘æ¨ç†å’Œé‡æ’åº
        try:
            # response = self.llm.generate_response(prompt)
            response = self.llm.generate_response(
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": query},
            ],
            response_format={"type": "json_object"},
        )
            log.info(f"LLM Re-ranking response: {response}")
            # å‡è®¾ LLM è¿”å› JSON æ ¼å¼çš„é‡æ’åºç»“æœ
            re_ranked_list: List[Dict[str, Any]] = json.loads(response.strip())
    
            # 3. ç»“æœæ•´åˆä¸æœ€ç»ˆè¾“å‡º (Final Integration)
            final_memories = []
            log.info(f"candidate_memories: {candidate_memories}")
            # åˆ›å»ºä¸€ä¸ª ID åˆ°åŸå§‹è®°å¿†çš„æ˜ å°„
            id_to_memory = {mem['memory_id']: mem for mem in candidate_memories}
            log.info(f"id_to_memory: {id_to_memory}")
            for item in re_ranked_list[:limit]: # é™åˆ¶æœ€ç»ˆè¾“å‡ºæ•°é‡
                mem_id = item.get('memory_id')
                log.info(f"Processing re-ranked memory ID: {mem_id}")
                if mem_id and mem_id in id_to_memory:
                    # æŸ¥æ‰¾åŸå§‹è®°å¿†æ–‡æœ¬å’Œåˆ†æ•°
                    original_memory = id_to_memory[mem_id]
                    final_memories.append({
                        "id": mem_id,
                        "text": original_memory['text'],
                        "score": original_memory['score'],
                        "rank_reasoning": item.get('reasoning') # åŒ…å«é‡æ’åºçš„é€»è¾‘è§£é‡Š
                    })
            log.info(f"Final re-ranked memories: {final_memories}")
            # å¦‚æœ LLM é‡æ’åºå¤±è´¥æˆ–è¿”å›ç©ºï¼Œåˆ™å›é€€åˆ°åŸå§‹å‘é‡æ£€ç´¢ç»“æœ
            if not final_memories and candidate_results:
                print("Warning: LLM re-ranking failed, falling back to top vector results.")
                return [{
                    "id": (getattr(res, 'id', None) or (res.payload.get('id') if isinstance(res.payload, dict) else None)),
                    "text": (res.payload.get('text') if isinstance(res.payload, dict) else getattr(res, 'text', '')),
                    "score": getattr(res, 'score', None),
                    "rank_reasoning": "Fallback (LLM re-ranking failure)"
                } for res in candidate_results[:limit]]
                
            return final_memories
            
        except Exception as e:
            print(f"Warning: LLM Re-ranking failed with error: {e}. Falling back to top vector results.")
            # å¤±è´¥å›é€€æœºåˆ¶
            return [{
                "id": (getattr(res, 'id', None) or (res.payload.get('id') if isinstance(res.payload, dict) else None)),
                "text": (res.payload.get('text') if isinstance(res.payload, dict) else getattr(res, 'text', '')),
                "score": getattr(res, 'score', None),
                "rank_reasoning": "Fallback (System Error)"
            } for res in candidate_results[:limit]]
    # delete, get, get_all, history, update
    def delete(self, user_id: str, memory_id: str) -> bool:
        # åˆ é™¤å‘é‡å­˜å‚¨ä¸­çš„è®°å¿†
        self.vector_store.delete(ids=[memory_id])
        
        # åˆ é™¤å†å²æ•°æ®åº“ä¸­çš„è®°å¿†
        self.db.delete_memory(user_id, memory_id)
        
        # åˆ é™¤å›¾å­˜å‚¨ä¸­çš„è®°å¿†èŠ‚ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_graph and self.graph:
            self.graph.delete_memory_node(memory_id)
    def get(self, user_id: str, memory_id: str) -> Optional[Dict[str, Any]]:
        return self.db.get_memory(user_id, memory_id)
    def get_all(self, user_id: str) -> List[Dict[str, Any]]:
        return self.db.get_all_memories(user_id)
    def history(self, user_id: str, limit: int = 10) -> List[Dict[str, Any]]:
        return self.db.get_recent_memories(user_id, limit)
    def update(self, user_id: str, memory_id: str, new_text: str, new_metadata: Optional[Dict[str, Any]] = None) -> bool:
        # æ›´æ–°å‘é‡å­˜å‚¨ä¸­çš„è®°å¿†
        new_embedding = self.embedding_model.embed(new_text)
        self.vector_store.update(
            ids=[memory_id],
            vectors=[new_embedding],
            payloads=[{"user_id": user_id, "text": new_text, **(new_metadata or {})}]
        ) 
# --- 4. Main æµ‹è¯•é€»è¾‘ ---
def main():
    config = {
    "llm": {
        "provider": "ollama",
        "config": {
            # "model": "qwen3:8b",
            "model": "gpt-oss:20b",
            "temperature": 0.1,
            "max_tokens": 4096,
        }
    },
        "embedder": {"provider": "huggingface", "config": {"model": "all-MiniLM-L6-v2"}},
    # "vector_store": {
    #     "provider": "qdrant",
    #     "config": {"collection_name": "vllm_memories", "host": "localhost", "port": 6333},
    # },
    #  "graph_store": {
    #         "provider": "neo4j",# or neo4j-community
    #         "config": {
    #             "url": "bolt://localhost:7687",
    #             "username": "neo4j", # or neo4j
    #             "password": "Neo4j2025",
    #             "database": "neo4j",
    #         }
    #     }
}
    print("--- ğŸ§  HRNSM è®°å¿†ç³»ç»Ÿå¯¹è¯æ–‡æœ¬æµ‹è¯• ---")
    memory = Memory.from_config(config)
    user_id = "test_user"

    # --- æµ‹è¯• ADD (å†™å…¥å¯¹è¯æ–‡æœ¬) ---
    print("\n## ğŸ“ 1. æµ‹è¯• ADD (å†™å…¥å¯¹è¯æ–‡æœ¬)")
    
    # ç¤ºä¾‹å¯¹è¯æ–‡æœ¬ 1: ä¸ªäººäº‹ä»¶
    dialogue_1 = "Melanie: Hey Caroline, since we last chatted, I've had a lot of things happening to me. I ran a charity race for mental health last Saturday â€“ it was really rewarding. Really made me think about taking care of our minds."
    memory.add(user_id, dialogue_1)
    
    # ç¤ºä¾‹å¯¹è¯æ–‡æœ¬ 2: å…³é”®äº‹å®
    dialogue_2 = "Melanie: The Q4 Report review is scheduled for next Monday. Caroline: Perfect, I'll block out time for that."
    new_id = memory.add(user_id, dialogue_2)
    
    print(f"-> å·²å­˜å‚¨ä¸¤ä¸ªå¯¹è¯ç‰‡æ®µ (ä¸€ä¸ªå…³äºæ…ˆå–„è·‘ï¼Œä¸€ä¸ªå…³äº Q4 æŠ¥å‘Š)")
    
    # éªŒè¯ç¬¦å·æå–æ˜¯å¦è¢«è°ƒç”¨
    # print(f"-> éªŒè¯å›¾å­˜å‚¨è°ƒç”¨: {memory.graph.add_memory_node.called}")

    print("\n---")
    
    # --- æµ‹è¯• SEARCH (æ£€ç´¢ï¼šé€»è¾‘è¿‡æ»¤å™ªå£°) ---
    print("## ğŸ” 2. æµ‹è¯• SEARCH (åŒé€šé“æ··åˆæ£€ç´¢ä¸é€»è¾‘è¿‡æ»¤)")
    # æŸ¥è¯¢ï¼šå¯»æ‰¾ä¸€ä¸ªæ˜ç¡®çš„æ—¥ç¨‹/äº‹å®
    query = "When is the Q4 Report review scheduled?"
    
    results = memory.search(user_id, query, limit=2)
    
    print(f"-> æœç´¢æŸ¥è¯¢: '{query}'")
    print(f"-> **æœ€ç»ˆç»“æœ (Top 2):**")
    
    if not results:
        print("æœç´¢å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœã€‚")
        return

    for i, result in enumerate(results):
        print(f"\n- Rank {i+1}: (Score: {result['score']})")
        print(f"  Text: {result['text']}")
        print(f"  Logic: {result['rank_reasoning']}")
        
    # é¢„æœŸåˆ†æï¼š
    # å‘é‡æ£€ç´¢ä¼šè¿”å› M2 (Q4) å’Œ M1 (æ…ˆå–„è·‘)
    # LLM é‡æ’åºä¼šåˆ¤æ–­ M2 ç›´æ¥å›ç­”äº†é—®é¢˜ï¼Œå°†å…¶æ’åœ¨ç¬¬ä¸€ï¼›M1 è™½è¯­ä¹‰ç›¸å…³ä½†ä¸æ˜¯äº‹å®ç­”æ¡ˆï¼Œæ’åœ¨ç¬¬äºŒã€‚
    # å™ªéŸ³ M3 (Q1 æŠ¥å‘Š) è¢« LLM é€»è¾‘è¿‡æ»¤ã€‚

if __name__ == "__main__":
    main()